{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup\n",
    "Set up the systerm with state detection, actions transition probability calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class myStates:\n",
    "    def __init__(self,Length,Width):\n",
    "        print(\"new state created with size %d X %d X 12...\" % (Length,Width))\n",
    "        self.L = Length\n",
    "        self.W = Width\n",
    "        self.sz = Length*Width*12\n",
    "        if self.L <= 0 or self.W <= 0:\n",
    "            raise Exception('Dimension of state should be positive integers')\n",
    "        \n",
    "        self.stateMatrix = []\n",
    "        # Create state list\n",
    "        dir_mat = np.array(range(12))\n",
    "        for i in range(self.L):\n",
    "            for j in range(self.W):\n",
    "                for k in dir_mat:\n",
    "                    self.stateMatrix.append((i,j,k))\n",
    "                    \n",
    "# Creating Actions\n",
    "class myActions:\n",
    "    def __init__(self,act = None,turn = None):\n",
    "        self.actionMat = (act,turn)\n",
    "        self.sz = 7\n",
    "        self.Set = list()\n",
    "        self.Set.append((0,0))\n",
    "        self.Set.append((1,0))\n",
    "        self.Set.append((1,1))\n",
    "        self.Set.append((1,-1))\n",
    "        self.Set.append((-1,0))\n",
    "        self.Set.append((-1,1))\n",
    "        self.Set.append((-1,-1))\n",
    "        # 0: (0,0): Stay still\n",
    "        # 1: (1,0): Forward only\n",
    "        # 2: (1,1): Forward clockwise\n",
    "        # 3: (1,-1): Forward counter-clockwise\n",
    "        # 4: (-1,0): Backward only\n",
    "        # 5: (-1,1): Backward clockwise\n",
    "        # 6: (-1,-1): Backward counter-clockwise\n",
    "        print('action setting up done...')\n",
    "        \n",
    "# Creating Probability Space functions\n",
    "def transitionProbability(pe,s,a,s_next,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: this function takes error probability pe, current state s, action and future\n",
    "                state as inputs, returns the transition probability between each state\n",
    "                \n",
    "    Input: pe = error probability, s = (x,y,h), a = (heading,rotation), s_next = (x',y',h'), \n",
    "            myStates = class of defined states\n",
    "            \n",
    "    Output: p = probability \n",
    "\n",
    "    \"\"\"\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    # pe threshold\n",
    "    if pe > 0.5 or pe < 0.0:\n",
    "        raise Exception('Error probability should lie between 0 and 0.5')\n",
    "    \n",
    "    # define possible cartesian movement\n",
    "    pos_x = [1,0]\n",
    "    pos_y = [0,1]\n",
    "    neg_x = [-1,0]\n",
    "    neg_y = [0,-1]\n",
    "    \n",
    "    # create a dictionary for possible heading direction based on current heading,\n",
    "    # consisting of three possible heading configuration for next state\n",
    "    \n",
    "    # h_dic[h] = [(moving_direction,h',possibility),~,~]\n",
    "    h_dic = {}\n",
    "    \n",
    "    h_dic[0] = [(pos_y,0,1-2*pe),(pos_y,1,pe),(pos_x,11,pe)]\n",
    "    h_dic[1] = [(pos_y,1,1-2*pe),(pos_x,2,pe),(pos_x,0,pe)]\n",
    "    h_dic[2] = [(pos_x,2,1-2*pe),(pos_x,3,pe),(pos_x,1,pe)]\n",
    "    h_dic[3] = [(pos_x,3,1-2*pe),(pos_x,4,pe),(neg_y,2,pe)]\n",
    "    h_dic[4] = [(pos_x,4,1-2*pe),(neg_y,5,pe),(neg_y,3,pe)]\n",
    "    h_dic[5] = [(neg_y,5,1-2*pe),(neg_y,6,pe),(neg_y,4,pe)]\n",
    "    h_dic[6] = [(neg_y,6,1-2*pe),(neg_y,7,pe),(neg_x,5,pe)]\n",
    "    h_dic[7] = [(neg_y,7,1-2*pe),(neg_x,8,pe),(neg_x,6,pe)]\n",
    "    h_dic[8] = [(neg_x,8,1-2*pe),(neg_x,9,pe),(neg_x,7,pe)]\n",
    "    h_dic[9] = [(neg_x,9,1-2*pe),(neg_x,10,pe),(pos_y,8,pe)]\n",
    "    h_dic[10] = [(neg_x,10,1-2*pe),(pos_y,11,pe),(pos_y,9,pe)]\n",
    "    h_dic[11] = [(pos_y,11,1-2*pe),(pos_y,0,pe),(pos_y,10,pe)]\n",
    "    \n",
    "    \n",
    "    # create a dictionary for transition probability based on future state \n",
    "    # and current state\n",
    "    transProb = {}\n",
    "\n",
    "    for map_key in h_dic[s[2]]:\n",
    "        x_new = s[0] + a[0]*map_key[0][0]   # move in x direction, a[0] indicates forward or backward\n",
    "        xd = x_new if (x_new <= L-1 and x_new >= 0) else s[0]       # else for off-grid movement\n",
    "        y_new = s[1] + a[0]*map_key[0][1]   # move in y direction, a[0] indicates forward or backward\n",
    "        yd = y_new if (y_new <= W-1 and y_new >= 0) else s[1]       # else for off-grid movement\n",
    "        hd = (map_key[1] + a[1]) % 12       # new heading direction\n",
    "        if a[0] == 0 and a[1] == 0:\n",
    "            transProb[s] = 1\n",
    "        else: transProb[(xd,yd,hd)] = map_key[2]\n",
    "    \n",
    "    # match with the keys in transProb dictionary\n",
    "    if s_next in transProb.keys():\n",
    "       # print(\"p = %f\" %(transProb[s_next]))\n",
    "        return(transProb[s_next]);\n",
    "    else: \n",
    "       # print(\"p = 0\")\n",
    "        return 0.0\n",
    "    \n",
    "        \n",
    "\n",
    "# update state based on action and current state\n",
    "def stateUpdate(pe,s,a,myStates):\n",
    "    \"\"\"\n",
    "    Description: This function performs updating of current state s based on action it takes and error\n",
    "                probability when doing action\n",
    "                \n",
    "    Input: pe = error probability, s = (x,y,h), a = (heading,direction), myStates = class of defined states\n",
    "    \n",
    "    Output: s_next = (x',y',h')\n",
    "    \"\"\"\n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    P = []\n",
    "    # search for probability trasferring to state s_next given current state and action\n",
    "    for s_next in S:\n",
    "        pt = transitionProbability(pe,s,a,s_next,myStates)\n",
    "        if pt != 0:\n",
    "            P.append((s_next,pt))\n",
    "    \n",
    "    prob = np.array([])\n",
    "    for p in P:\n",
    "        prob = np.append(prob,p[1])\n",
    "\n",
    "    # return a choice given discrete pdf\n",
    "    state_id = np.random.choice(np.arange(len(P)),p=prob)\n",
    "    s_next = P[state_id][0]\n",
    "    return(s_next)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new state created with size 6 X 6 X 12...\n",
      "action setting up done...\n",
      "The size of {S} Ns is L X W X 12, for a 6 X 6 grid, Ns is 432\n",
      "The size of {A} Na is 7\n"
     ]
    }
   ],
   "source": [
    "S = myStates(6,6)\n",
    "A = myActions()\n",
    "StateMatrix = S.stateMatrix\n",
    "ActionSet = A.Set\n",
    "\n",
    "print(\"The size of {S} Ns is L X W X 12, for a 6 X 6 grid, Ns is %d\" %S.sz)\n",
    "print(\"The size of {A} Na is %d\" %A.sz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reward map for state input\n",
    "def rewardFun(s,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: This function takes a certain state in state map and output the reward at that state\n",
    "    \n",
    "    Input: s = current state, myStates = class of defined states\n",
    "    \n",
    "    Return: float(reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from states\n",
    "    S = myStates.stateMatrix\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    \n",
    "    \n",
    "    x_pos = s[0]\n",
    "    y_pos = s[1]\n",
    "    h = s[2]\n",
    "    \n",
    "    if x_pos < 0 or x_pos >= L or y_pos < 0 or y_pos >= W or h < 0 or h >= 12:\n",
    "        raise Exception('Invalid state definition: [x,y,h] should be within range')\n",
    "    \n",
    "    pos = [x_pos,y_pos]\n",
    "    \n",
    "    if x_pos == 0 or y_pos == 0 or x_pos == (L-1) or y_pos == (W-1):\n",
    "        r = -100\n",
    "    elif pos == [2,2] or pos == [2,3] or pos == [2,4] or pos == [4,2] or pos == [4,3] or pos == [4,4]:\n",
    "        r = -1\n",
    "    elif pos == [3,4]:\n",
    "        r = 1\n",
    "    else: r = 0\n",
    "     \n",
    "    # print(\"reward for state (%d,%d,%d) is %d\" %(s[0],s[1],s[2],r))\n",
    "    return r\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rwd_map = []\n",
    "i = 0\n",
    "\n",
    "for s in StateMatrix:\n",
    "    if not(i % 12):\n",
    "        rwd_map.append(rewardFun(s,S))\n",
    "    i = i + 1\n",
    "\n",
    "rwd_map = np.asarray(rwd_map)\n",
    "rwd = rwd_map.reshape(6,6).transpose()\n",
    "fig = plt.imshow(rwd,origin = \"lower\",vmin = -10,vmax = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyInitialize(myStates,myActions):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function sets up a shortest-path based initial policy based on state and\n",
    "                action settings. It will generate a path to get the robot closest to goal for each\n",
    "                state. It only cares about the goal and it ignores all penalties (negative rewards)\n",
    "                in the map\n",
    "    \n",
    "    input: myStates = class of defined states, myActions = class of defined Actions\n",
    "    return: list(initial policy for each state)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    \n",
    "    \n",
    "    # initializing policy\n",
    "    p0 = {}\n",
    "    rewardState = []\n",
    "    p_init = []\n",
    "    for s in S:\n",
    "        if rewardFun(s,myStates) == 1:\n",
    "            rewardState.append(s)\n",
    "            p0[s] = (0,0)\n",
    "    \n",
    "    \n",
    "    # initialize policy based on shortest path\n",
    "    for s in S:\n",
    "        if s not in rewardState:\n",
    "            x_rwd = rewardState[0][0]\n",
    "            y_rwd = rewardState[0][1]\n",
    "            xs = np.copy(s[0])\n",
    "            ys = np.copy(s[1])\n",
    "            hs = np.copy(s[2])\n",
    "            stateMove = [np.sign(x_rwd - xs),np.sign(y_rwd - ys)]\n",
    "            if hs in np.array([11,0,1]):\n",
    "                p0[s] = (stateMove[1],stateMove[0])\n",
    "            elif hs in np.array([2,3,4]):\n",
    "                p0[s] = (stateMove[0],-stateMove[1])\n",
    "            elif hs in np.array([5,6,7]):\n",
    "                p0[s] = (-stateMove[1],stateMove[0])\n",
    "            else:\n",
    "                p0[s] = (-stateMove[0],-stateMove[1])\n",
    "            \n",
    "            # if reward lies next to the current state but not in travel direction, go backward\n",
    "            if p0[s] == (0,1) or p0[s] == (0,-1):\n",
    "                p0[s] = (-1,0)\n",
    "        \n",
    "        p_init.append(A.index(p0[s]))\n",
    "        \n",
    "    return(p_init)\n",
    "        \n",
    "\n",
    "def trajectoryFun(pe,pi,s,myStates,myActions):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function generates a trajectory to achieve goal based on input state\n",
    "                and input policy\n",
    "    \n",
    "    input: pe = error probability, pi = [policy for each state], s = current state, myState\n",
    "            = class of defined states, myActions = class of defined actions\n",
    "    return: [trajectory for input state], plot of trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    state_action = {}\n",
    "    s_traj = []\n",
    "    i = 0\n",
    "    \n",
    "    # get trajectory updated for every action done based in input state\n",
    "    while rewardFun(s,myStates) <= 0:\n",
    "        i = S.index(s)\n",
    "        state_action[s] = pi[i]\n",
    "        s_new = stateUpdate(pe,s,A[pi[i]],myStates)\n",
    "        s_traj.append(s_new)\n",
    "        s = s_new\n",
    "        \n",
    "    \n",
    "    # plot trajectory\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    i = 0\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for s in s_traj:\n",
    "        xs.append(s[0])\n",
    "        ys.append(s[1])\n",
    "        \n",
    "    plt.plot(xs,ys)\n",
    "    plt.axis([0,L-1,0,W-1])\n",
    "    plt.grid(True)\n",
    "        \n",
    "    return(s_traj)\n",
    "\n",
    "\n",
    "def valueFun(pe,pi,gamma,myStates,myActions):\n",
    "    \"\"\"\n",
    "    Description: This function calculates value of each state for a given policy and discounts\n",
    "    \n",
    "    Input: pe = error probability, pi = list(given policy for all states), gamma = discounts\n",
    "    \n",
    "    Output: list(values for each state)\n",
    "    \"\"\"\n",
    "    if gamma > 1 or gamma < 0:\n",
    "        raise Exception(\"Invalid input. gamma should be in range[0,1])\")\n",
    "    \n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    V = {}\n",
    "    i = 0\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    for s in S:\n",
    "        V[s] = rewardFun(s,myStates)+sum(transitionProbability(pe,s,A[pi[i]],s2,myStates)*V[s2]*gamma for s2 in S)\n",
    "        i = i + 1\n",
    "        \n",
    "    return(V)\n",
    "    \n",
    "    \n",
    "        \n",
    "def policyIteration(pe,pol,gamma,myStates,myActions):\n",
    "    \"\"\"\n",
    "    Description: This function use policy iteration to get an optimal policy based on initialized policy\n",
    "    \n",
    "    input: pe = error_probability, myStates = class of defined states, myAction = class of \n",
    "            defined actions, gamma = time_discount, epsilon = convergence threshold\n",
    "    \n",
    "    return: list(policy,value)\n",
    "    \n",
    "    Note: This function runs slowly. Please be patient\n",
    "    \"\"\"\n",
    "    if gamma > 1 or gamma < 0:\n",
    "        raise Exception(\"Invalid input. gamma should be in range[0,1])\")\n",
    "    \n",
    "    \n",
    "    start = timer()\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    V = {}\n",
    "    pi = np.copy(pol)\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    # initialize revised policy to enter in loop\n",
    "    pi_optimal = np.copy(pi)-np.copy(pi)\n",
    "    ct = 0\n",
    "    while any(pi - pi_optimal):\n",
    "        i = 0\n",
    "        pi_optimal = np.copy(pi)\n",
    "        for s in S:\n",
    "            value_action = []\n",
    "            for a in range(0,7):\n",
    "                # append value_action list for state s for all actions\n",
    "                value_action.append(rewardFun(s,myStates)+gamma*sum(\n",
    "                    transitionProbability(pe,s,A[a],s2,myStates)*V[s2] for s2 in S))\n",
    "                    \n",
    "            # set policy with highest value\n",
    "            pi[i] = np.argmax(value_action)\n",
    "            V[s] = max(value_action)\n",
    "            i = i + 1\n",
    "      \n",
    "        ct = ct + 1\n",
    "        print(\"policy iterates for %d times......\" %(ct))\n",
    "        \n",
    "    end = timer()\n",
    "    \n",
    "    print(\"policy iteration done in %f seconds\" %(end-start))\n",
    "        \n",
    "    return(pi_optimal,V)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial policy setting up with action ID...\n",
      "[2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 6, 6, 5, 5, 5, 3, 3, 3, 2, 2, 2, 6, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 6, 6, 5, 5, 5, 3, 3, 3, 2, 2, 2, 6]\n",
      "trajectory for robot using initialized policy: \n",
      "[(1, 5, 6), (1, 4, 7), (1, 5, 7), (1, 4, 8), (2, 4, 8), (3, 4, 8)]\n",
      "values along initialized trajectory: -100.000000,0.000000,-100.000000,0.000000,-1.000000,1.000000\n",
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iteration done in 66.641317 seconds\n",
      "optimized policy is: \n",
      "[5 2 3 2 2 6 3 5 6 5 5 3 2 2 1 1 1 6 5 5 4 4 4 3 2 2 3 1 2 6 5 5 6 4 5 3 2\n",
      " 2 3 1 2 6 2 5 6 4 5 3 2 5 1 1 1 3 2 2 4 4 4 6 2 5 3 2 2 3 6 2 6 5 5 6 1 2\n",
      " 3 2 2 6 4 5 6 5 5 3 1 1 1 2 1 4 4 4 4 5 4 1 2 1 1 2 1 4 5 4 4 5 4 1 1 2 1\n",
      " 2 1 6 1 5 4 5 4 3 5 4 1 1 1 1 2 1 4 4 4 4 4 5 3 2 2 3 1 2 6 5 5 6 2 2 3 2\n",
      " 2 6 5 5 6 2 5 3 2 2 3 1 2 6 5 5 6 1 5 3 2 2 3 1 2 6 5 5 6 1 5 3 2 2 3 1 2\n",
      " 6 2 5 6 1 5 3 5 5 1 1 1 3 2 2 4 4 4 6 5 5 3 2 2 3 2 2 6 2 5 6 1 1 3 2 2 4\n",
      " 4 4 3 2 2 1 1 1 1 2 1 4 4 4 1 2 1 1 1 1 1 2 1 4 4 4 1 2 1 1 1 1 1 2 1 4 4\n",
      " 4 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 4 4 3 2 2 1 1 1 3 2 2 4 2 2 6 2 5 6 5 5\n",
      " 3 2 2 3 2 2 6 4 5 6 5 5 3 1 2 3 2 2 6 4 5 6 5 5 3 1 2 3 2 2 6 4 5 6 2 5 3\n",
      " 1 2 3 5 5 4 4 4 3 2 2 1 1 1 6 5 5 6 2 5 3 2 2 3 2 2 6 5 2 6 5 5 6 3 5 3 2\n",
      " 2 3 2 2 4 5 4 6 5 5 1 2 1 3 2 2 4 5 4 6 5 5 1 2 1 3 2 2 4 5 4 6 2 5 1 2 1\n",
      " 3 2 5 4 4 4 3 2 2 1 1 1 6 2 5 6 5 5 3 6 2 3 2 2 6]\n",
      "trajectory for optimized policy: \n",
      "[(1, 3, 7), (1, 4, 8), (2, 4, 8), (3, 4, 8)]\n",
      "values for optimized policy: \n",
      "2.9932155990000004\n",
      "3.713215599\n",
      "4.5132155990000005\n",
      "6.5132155990000005\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialized policy\n",
    "p_init = policyInitialize(S,A)\n",
    "print(\"initial policy setting up with action ID...\")\n",
    "print(p_init)\n",
    "print(\"trajectory for robot using initialized policy: \")\n",
    "s_traj = trajectoryFun(0,p_init,(1,4,6),S,A)\n",
    "print(s_traj)\n",
    "V = valueFun(0,p_init,0.9,S,A)\n",
    "print(\"values along initialized trajectory: %f,%f,%f,%f,%f,%f\" \n",
    "      %(V[(1,5,6)],V[(1,4,7)],V[(1,5,7)],V[(1,4,8)],V[(2,4,8)],V[(3,4,8)]))\n",
    "\n",
    "# Optimized policy and value from policy iteration\n",
    "pi_optimal,v_optimal = policyIteration(0,p_init,0.9,S,A)\n",
    "print(\"optimized policy is: \")\n",
    "print(pi_optimal)\n",
    "\n",
    "\n",
    "# Optimized Trajectory\n",
    "s_traj_opt = trajectoryFun(0,pi_optimal,(1,4,6),S,A)\n",
    "print(\"trajectory for optimized policy: \")\n",
    "print(s_traj_opt)\n",
    "print(\"values for optimized policy: \")\n",
    "for s in v_optimal.keys():\n",
    "    if s in s_traj_opt:\n",
    "        print(v_optimal[s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(pe,myStates,myActions,gamma,epsilon=0.5):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function takes error probability, action, current state,\n",
    "                next state, state map and time discount as input and returns the optimal \n",
    "                policy for each state as well as value for each state\n",
    "    \n",
    "    input: pe = error_probability, myStates = class of defined states, myAction = class of \n",
    "            defined actions, gamma = time_discount, epsilon = convergence threshold\n",
    "    return: list(policy,value)\n",
    "    \n",
    "    NOTE: 1.This function runs slowly as not optimized for efficiency, please be patient\n",
    "          2. epsilon is usually set to be 0.01 and default value 0.5 used for demo\n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    if epsilon > 0.5 or epsilon < 0.0:\n",
    "        raise Exception(\"Invalid epsilon. Enter a value between 0 and 0.5\")\n",
    "        \n",
    "        \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    # initializae all states' values\n",
    "    V = {}\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    pi = {}\n",
    "    max_value = {}\n",
    "        \n",
    "        \n",
    "    # set a large threshold bigger than epsilon to enter in the loop\n",
    "    delta = 10000\n",
    "    \n",
    "    # perform value iteration\n",
    "    while delta > epsilon:\n",
    "        \n",
    "        # initialize delta to be zero, construct empty list for policy and value\n",
    "        delta = 0\n",
    "        policy = []\n",
    "        value = []\n",
    "        \n",
    "        # iteration for agent being at state s in S\n",
    "        for s in S:\n",
    "            v = np.copy(V[s])\n",
    "            value_action = []\n",
    "            \n",
    "            # for all actions, find value for action a at state s, sum over all future state s2\n",
    "            for a in range(0,7):\n",
    "                value_action.append(sum(transitionProbability(pe,s,A[a],s2,myStates)*(rewardFun(s2,myStates)+gamma*V[s2]) for s2 in S))\n",
    "            V[s] = max(value_action)\n",
    "            pi[s] = np.argmax(value_action)\n",
    "            \n",
    "            # construct policy list and value list\n",
    "            policy.append(pi[s])\n",
    "            value.append(V[s])\n",
    "            \n",
    "            # update delta for each state\n",
    "            delta = max(delta,abs(v-V[s]))\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Value iteration costs %f seconds.......\" %(end - start))\n",
    "\n",
    "    return(policy,value)\n",
    "\n",
    "\n",
    "def mappingState(policy,value,myStates,myActions):\n",
    "    A = myActions.Set\n",
    "    S = myStates.stateMatrix\n",
    "    \n",
    "    # convert input list to array\n",
    "    V = np.asarray(value)\n",
    "    P = np.asarray(policy)\n",
    "    \n",
    "    vmap = {}\n",
    "    pmap = {}\n",
    "    \n",
    "    \n",
    "    # mapping state with policy and value\n",
    "    ct = 0\n",
    "    for s in S:\n",
    "        vmap[s] = V[ct]\n",
    "        pmap[s] = A[P[ct]]\n",
    "        ct = ct + 1\n",
    "        \n",
    "    return(vmap,pmap)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration costs 63.326981 seconds.......\n",
      "optimal policy using value iteration is: \n",
      "[5, 2, 3, 2, 2, 6, 3, 5, 6, 5, 5, 3, 2, 2, 1, 1, 1, 6, 5, 5, 4, 4, 4, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 4, 5, 3, 2, 2, 3, 1, 2, 6, 2, 5, 6, 4, 5, 3, 2, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 2, 5, 3, 2, 2, 3, 6, 2, 6, 5, 5, 6, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 1, 1, 2, 1, 4, 4, 4, 4, 5, 4, 1, 2, 1, 1, 2, 1, 4, 5, 4, 4, 5, 4, 1, 1, 2, 1, 2, 1, 6, 1, 5, 4, 5, 4, 3, 5, 4, 1, 1, 1, 1, 2, 1, 4, 4, 4, 4, 4, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 2, 2, 3, 2, 2, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 1, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 1, 5, 3, 2, 2, 3, 1, 2, 6, 2, 5, 6, 1, 5, 3, 5, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 2, 5, 6, 1, 1, 3, 2, 2, 4, 4, 4, 3, 2, 2, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 2, 2, 1, 1, 1, 3, 2, 2, 4, 2, 2, 6, 2, 5, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 2, 5, 3, 1, 2, 3, 5, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 2, 2, 6, 5, 2, 6, 5, 5, 6, 3, 5, 3, 2, 2, 3, 2, 2, 4, 5, 4, 6, 5, 5, 1, 2, 1, 3, 2, 2, 4, 5, 4, 6, 5, 5, 1, 2, 1, 3, 2, 2, 4, 5, 4, 6, 2, 5, 1, 2, 1, 3, 2, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 2, 5, 6, 5, 5, 3, 6, 2, 3, 2, 2, 6]\n",
      "comparison with policy iteration: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_optimal_v,v_optimal = ValueIteration(0,S,A,0.9)\n",
    "print(\"optimal policy using value iteration is: \")\n",
    "print(pi_optimal_v)\n",
    "\n",
    "print(\"comparison with policy iteration: \")\n",
    "print(pi_optimal_v - pi_optimal)\n",
    "s_traj_v = trajectoryFun(0,pi_optimal_v,(1,4,6),S,A)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recomputed trajectory with pe = 0.25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_traj_v = trajectoryFun(0.25,p_init,(1,4,6),S,A)\n",
    "print(\"recomputed trajectory with pe = 0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardFun(s,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: This function takes a certain state in state map and output the reward at that state\n",
    "    \n",
    "    Input: s = current state, myStates = class of defined states\n",
    "    \n",
    "    Return: float(reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from states\n",
    "    S = myStates.stateMatrix\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    \n",
    "    \n",
    "    x_pos = s[0]\n",
    "    y_pos = s[1]\n",
    "    h = s[2]\n",
    "    \n",
    "    if x_pos < 0 or x_pos >= L or y_pos < 0 or y_pos >= W or h < 0 or h >= 12:\n",
    "        raise Exception('Invalid state definition: [x,y,h] should be within range')\n",
    "    \n",
    "    pos = [x_pos,y_pos]\n",
    "    \n",
    "    if x_pos == 0 or y_pos == 0 or x_pos == (L-1) or y_pos == (W-1):\n",
    "        r = -100\n",
    "    elif pos == [2,2] or pos == [2,3] or pos == [2,4] or pos == [4,2] or pos == [4,3] or pos == [4,4]:\n",
    "        r = -1\n",
    "        \n",
    "    # modification to original rewardFun\n",
    "    elif pos == [3,4] and h in [6,7,8]:\n",
    "        r = 1\n",
    "    else: r = 0\n",
    "     \n",
    "    # print(\"reward for state (%d,%d,%d) is %d\" %(s[0],s[1],s[2],r))\n",
    "    return r\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iterates for 11 times......\n",
      "policy iterates for 12 times......\n",
      "policy iterates for 13 times......\n",
      "policy iteration done in 85.151652 seconds\n",
      "recomputed trajectory using new reward function: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pi_optimal_add,v_optimal_add = policyIteration(0,p_init,0.9,S,A)\n",
    "s_traj_v = trajectoryFun(0,pi_optimal_add,(1,4,6),S,A)\n",
    "print(\"recomputed trajectory using new reward function: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Theoretically, both value iteration process and policy iteration process will converges the optimal policy and optimal value to the same quantities. If the number of states are not large, their computational time will not have big difference\n",
    "\n",
    "For current reward +1 in map, it might not be intuitive that the robot will not go along a path with zero reward and then reach +1 in map, the truth is for all above situations, the time discount is less than 1 and the obstacle grid only have -1 reward in map, which will not stop the robot travelling by short-cut. If we change our obstacle grid with -100 reward in map, we will get new trajectory looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iteration done in 64.540024 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAC0RJREFUeJzt3N+LZwd5x/HPkx8lMaYI7VSiUVJoGSpCTRvsRaBMg5VUxfauCnol7E1bIi1Ivcw/IN5IcFFpi6kiaKCm1DagXyQYf22MNsm6QWxoQ4TFtoMZCGkzeXox33Qkzux8s5nZb57d1wuGzOye3Tw8nH3v4cw5W90dAOa4at0DAPDyCDfAMMINMIxwAwwj3ADDCDfAMNesclBVPZnkmSS7SZ7v7ttOcigADrdSuJf+oLt/emKTALASt0oAhqlV3pysqn9L8t9JOsknu/v0AcecSnIqSa677rrfffOb33zMo870wgsv5Kqr/P1oD/vsYp9d7HviiSd+2t0bqxy7arjf0N1PV9WvJXkgyV9099cPO35zc7PPnTu38sCXs8Vika2trXWPsXb2sM8u9tnFvqo6s+r3D1f6q667n17+93yS+5K8/eLHA+CVODLcVXVDVd344udJ3pnk0ZMeDICDrfJUyeuT3FdVLx7/9939lROdCoBDHRnu7v5xkt++BLMAsALfzgUYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGGEG2AY4QYYRrgBhhFugGFWDndVXV1V36uq+09yIAAu7OVccd+V5OxJDQLAalYKd1XdnOTdST51suPA5e3uLz+We88+t+4xGO6aFY/7eJKPJLnxsAOq6lSSU0mysbGRxWLxioe7HOzs7NhF7OFF33j82ezu7trFkvPi4hwZ7qp6T5Lz3X2mqrYOO667Tyc5nSSbm5u9tXXooVeUxWIRu7CHF91z7qFsb2/bxZLz4uKscqvk9iTvraonk3w+yR1V9dkTnQqAQx0Z7u7+aHff3N23JHlfkq929wdOfDIADuQ5boBhVv3mZJKkuxdJFicyCQArccUNMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwxzZLir6rqq+nZVfb+qHququy/FYAAc7JoVjnkuyR3dvVNV1yZ5sKr+qbu/ecKzAXCAI6+4e8/O8strlx99olNdJu7+8mO59+xz6x4DuMyscsWdqro6yZkkv5HkE939rQOOOZXkVJJsbGxksVgc45gzfePxZ7O7u2sXSXZ2duwhyfa2c+LnOS8uzkrh7u7dJG+rqtclua+q3trdj77kmNNJTifJ5uZmb21tHfes49xz7qFsb2/HLpLFYmEPcU68lPPi4rysp0q6ezvJIsmdJzINAEda5amSjeWVdqrq+iTvSPLDkx4MgIOtcqvkpiR/u7zPfVWSL3T3/Sc7FgCHOTLc3f2DJLdeglkAWIE3JwGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYJgjw11Vb6qqr1XV2ap6rKruuhSDAXCwa1Y45vkkf9XdD1fVjUnOVNUD3f34Cc8GwAGOvOLu7p9098PLz59JcjbJG096MAAOtsoV9/+rqluS3JrkWwf83Kkkp5JkY2Mji8XilU833Pb2s9nd3bWLJDs7O/YQ58RLOS8uzsrhrqrXJvlikg93989e+vPdfTrJ6STZ3Nzsra2t45pxrHvOPZTt7e3YRbJYLOwhzomXcl5cnJWeKqmqa7MX7Xu7+0snOxIAF7LKUyWV5NNJznb3x05+JAAuZJUr7tuTfDDJHVX1yPLjXSc8FwCHOPIed3c/mKQuwSwArMCbkwDDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMMINMIxwAwwj3ADDCDfAMEeGu6o+U1Xnq+rRSzEQABe2yhX33yS584TnAGBF1xx1QHd/vapuOflRLk///swL+dNPPrTuMdZue/vZ3HPOHh7/yc/yhuvXPQXTHRnuVVXVqSSnkmRjYyOLxeK4fuuxfus1/5v/fE1ne3t73aOs3e7urj0kecP1ya2/suvPx9LOzo5dXITq7qMP2rvivr+737rKb7q5udnnzp17ZZNdJhaLRba2ttY9xtrZwz672GcX+6rqTHfftsqxnioBGEa4AYZZ5XHAzyV5KMlmVT1VVR86+bEAOMwqT5W8/1IMAsBq3CoBGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhVgp3Vd1ZVeeq6kdV9dcnPRQAhzsy3FV1dZJPJPmjJG9J8v6qestJDwbAwVa54n57kh9194+7+3+SfD7JH5/sWAAc5poVjnljkv/4ua+fSvJ7Lz2oqk4lObX88rmqevSVj3dZ+NUkP133EK8C9rDPLvbZxb7NVQ9cJdx1wI/1L/xA9+kkp5Okqr7b3betOsTlzC722MM+u9hnF/uq6rurHrvKrZKnkrzp576+OcnTL3coAI7HKuH+TpLfrKpfr6pfSvK+JP9wsmMBcJgjb5V09/NV9edJ/jnJ1Uk+092PHfHLTh/HcJcJu9hjD/vsYp9d7Ft5F9X9C7erAXgV8+YkwDDCDTDMsYbbq/F7quozVXXes+xJVb2pqr5WVWer6rGqumvdM61LVV1XVd+uqu8vd3H3umdat6q6uqq+V1X3r3uWdaqqJ6vqX6vqkVUeCzy2e9zLV+OfSPKH2XuE8DtJ3t/djx/L/2CQqvr9JDtJ/q6737ruedapqm5KclN3P1xVNyY5k+RPrtDzopLc0N07VXVtkgeT3NXd31zzaGtTVX+Z5LYkv9zd71n3POtSVU8mua27V3oZ6TivuL0av9TdX0/yX+ue49Wgu3/S3Q8vP38mydnsvY17xek9O8svr11+XLFPB1TVzUneneRT655lmuMM90Gvxl+Rf0A5WFXdkuTWJN9a7yTrs7w18EiS80ke6O4rdhdJPp7kI0leWPcgrwKd5F+q6szynw+5oOMM90qvxnNlqqrXJvlikg9398/WPc+6dPdud78te28gv72qrshbaVX1niTnu/vMumd5lbi9u38ne/8K658tb7ce6jjD7dV4DrS8n/vFJPd295fWPc+rQXdvJ1kkuXPNo6zL7Uneu7y3+/kkd1TVZ9c70vp099PL/55Pcl/2bj0f6jjD7dV4fsHyG3KfTnK2uz+27nnWqao2qup1y8+vT/KOJD9c71Tr0d0f7e6bu/uW7LXiq939gTWPtRZVdcPyG/epqhuSvDPJBZ9IO7Zwd/fzSV58Nf5ski+s8Gr8ZamqPpfkoSSbVfVUVX1o3TOt0e1JPpi9K6pHlh/vWvdQa3JTkq9V1Q+yd6HzQHdf0Y/BkSR5fZIHq+r7Sb6d5B+7+ysX+gVeeQcYxpuTAMMIN8Awwg0wjHADDCPcAMMIN8Awwg0wzP8BQdvwhHI/ixsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_optimal_add2,v_optimal_add2 = policyIteration(0,p_init,0.9,S,A)\n",
    "s_traj_v = trajectoryFun(0,pi_optimal_add2,(1,4,6),S,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
