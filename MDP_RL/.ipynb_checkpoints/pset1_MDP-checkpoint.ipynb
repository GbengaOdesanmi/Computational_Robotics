{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup\n",
    "Set up the systerm with state detection, actions transition probability calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class myStates:\n",
    "    def __init__(self,Length,Width):\n",
    "        print(\"new state created with size %d X %d X 12...\" % (Length,Width))\n",
    "        self.L = Length\n",
    "        self.W = Width\n",
    "        self.sz = Length*Width*12\n",
    "        if self.L <= 0 or self.W <= 0:\n",
    "            raise Exception('Dimension of state should be positive integers')\n",
    "        \n",
    "        self.stateMatrix = []\n",
    "        # Create state list\n",
    "        dir_mat = np.array(range(12))\n",
    "        for i in range(self.L):\n",
    "            for j in range(self.W):\n",
    "                for k in dir_mat:\n",
    "                    self.stateMatrix.append((i,j,k))\n",
    "                    \n",
    "# Creating Actions\n",
    "class myActions:\n",
    "    def __init__(self,act = None,turn = None):\n",
    "        self.actionMat = (act,turn)\n",
    "        self.sz = 7\n",
    "        self.Set = list()\n",
    "        self.Set.append((0,0))\n",
    "        self.Set.append((1,0))\n",
    "        self.Set.append((1,1))\n",
    "        self.Set.append((1,-1))\n",
    "        self.Set.append((-1,0))\n",
    "        self.Set.append((-1,1))\n",
    "        self.Set.append((-1,-1))\n",
    "        # 0: (0,0): Stay still\n",
    "        # 1: (1,0): Forward only\n",
    "        # 2: (1,1): Forward clockwise\n",
    "        # 3: (1,-1): Forward counter-clockwise\n",
    "        # 4: (-1,0): Backward only\n",
    "        # 5: (-1,1): Backward clockwise\n",
    "        # 6: (-1,-1): Backward counter-clockwise\n",
    "        print('action setting up done...')\n",
    "        \n",
    "# Creating Probability Space functions\n",
    "def transitionProbability(pe,s,a,s_next,myStates):\n",
    "    \n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    # this function takes error probability pe, current state s = (x,y,h), future\n",
    "    # state s_next = (x',y',h') and size of states (L,W) as inputs, returns the transition \n",
    "    # probability between each state\n",
    "    \n",
    "    # pe threshold\n",
    "    if pe > 0.5 or pe < 0.0:\n",
    "        raise Exception('Error probability should lie between 0 and 0.5')\n",
    "    \n",
    "    # define possible cartesian movement\n",
    "    pos_x = [1,0]\n",
    "    pos_y = [0,1]\n",
    "    neg_x = [-1,0]\n",
    "    neg_y = [0,-1]\n",
    "    \n",
    "    # create a dictionary for possible heading direction based on current heading,\n",
    "    # consisting of three possible heading configuration for next state\n",
    "    \n",
    "    # h_dic[h] = [(moving_direction,h',possibility),~,~]\n",
    "    h_dic = {}\n",
    "    \n",
    "    h_dic[0] = [(pos_y,0,1-2*pe),(pos_y,1,pe),(pos_x,11,pe)]\n",
    "    h_dic[1] = [(pos_y,1,1-2*pe),(pos_x,2,pe),(pos_x,0,pe)]\n",
    "    h_dic[2] = [(pos_x,2,1-2*pe),(pos_x,3,pe),(pos_x,1,pe)]\n",
    "    h_dic[3] = [(pos_x,3,1-2*pe),(pos_x,4,pe),(neg_y,2,pe)]\n",
    "    h_dic[4] = [(pos_x,4,1-2*pe),(neg_y,5,pe),(neg_y,3,pe)]\n",
    "    h_dic[5] = [(neg_y,5,1-2*pe),(neg_y,6,pe),(neg_y,4,pe)]\n",
    "    h_dic[6] = [(neg_y,6,1-2*pe),(neg_y,7,pe),(neg_x,5,pe)]\n",
    "    h_dic[7] = [(neg_y,7,1-2*pe),(neg_x,8,pe),(neg_x,6,pe)]\n",
    "    h_dic[8] = [(neg_x,8,1-2*pe),(neg_x,9,pe),(neg_x,7,pe)]\n",
    "    h_dic[9] = [(neg_x,9,1-2*pe),(neg_x,10,pe),(pos_y,8,pe)]\n",
    "    h_dic[10] = [(neg_x,10,1-2*pe),(pos_y,11,pe),(pos_y,9,pe)]\n",
    "    h_dic[11] = [(pos_y,11,1-2*pe),(pos_y,0,pe),(pos_y,10,pe)]\n",
    "    \n",
    "    \n",
    "    # create a dictionary for transition probability based on future state \n",
    "    # and current state\n",
    "    transProb = {}\n",
    "\n",
    "    for map_key in h_dic[s[2]]:\n",
    "        x_new = s[0] + a[0]*map_key[0][0]   # move in x direction, a[0] indicates forward or backward\n",
    "        xd = x_new if (x_new <= L-1 and x_new >= 0) else s[0]       # else for off-grid movement\n",
    "        y_new = s[1] + a[0]*map_key[0][1]   # move in y direction, a[0] indicates forward or backward\n",
    "        yd = y_new if (y_new <= W-1 and y_new >= 0) else s[1]       # else for off-grid movement\n",
    "        hd = (map_key[1] + a[1]) % 12       # new heading direction\n",
    "        if a[0] == 0 and a[1] == 0:\n",
    "            transProb[s] = 1\n",
    "        else: transProb[(xd,yd,hd)] = map_key[2]\n",
    "    \n",
    "    # match with the keys in transProb dictionary\n",
    "    if s_next in transProb.keys():\n",
    "       # print(\"p = %f\" %(transProb[s_next]))\n",
    "        return(transProb[s_next]);\n",
    "    else: \n",
    "       # print(\"p = 0\")\n",
    "        return 0.0\n",
    "    \n",
    "        \n",
    "\n",
    "# update state based on action and current state\n",
    "def stateUpdate(pe,s,a,myStates):\n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    P = []\n",
    "    # search for probability trasferring to state s_next given current state and action\n",
    "    for s_next in S:\n",
    "        pt = transitionProbability(pe,s,a,s_next,myStates)\n",
    "        if pt != 0:\n",
    "            P.append((s_next,pt))\n",
    "    \n",
    "    prob = np.array([])\n",
    "    for p in P:\n",
    "        prob = np.append(prob,p[1])\n",
    "\n",
    "    # return a choice given discrete pdf\n",
    "    state_id = np.random.choice(np.arange(len(P)),p=prob)\n",
    "    s_next = P[state_id][0]\n",
    "    return(s_next)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new state created with size 6 X 6 X 12...\n",
      "action setting up done...\n",
      "The size of {S} Ns is L X W X 12, for a 6 X 6 grid, Ns is 432\n",
      "The size of {A} Na is 7\n"
     ]
    }
   ],
   "source": [
    "S = myStates(6,6)\n",
    "A = myActions()\n",
    "StateMatrix = S.stateMatrix\n",
    "ActionSet = A.Set\n",
    "\n",
    "print(\"The size of {S} Ns is L X W X 12, for a 6 X 6 grid, Ns is %d\" %S.sz)\n",
    "print(\"The size of {A} Na is %d\" %A.sz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reward map for state input\n",
    "def rewardFun(s,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: This function takes a certain state in state map and output the reward at that state\n",
    "    \n",
    "    Input: s = current state, myStates = class of defined states\n",
    "    \n",
    "    Return: float(reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from states\n",
    "    S = myStates.stateMatrix\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    \n",
    "    \n",
    "    x_pos = s[0]\n",
    "    y_pos = s[1]\n",
    "    h = s[2]\n",
    "    \n",
    "    if x_pos < 0 or x_pos >= L or y_pos < 0 or y_pos >= W or h < 0 or h >= 12:\n",
    "        raise Exception('Invalid state definition: [x,y,h] should be within range')\n",
    "    \n",
    "    pos = [x_pos,y_pos]\n",
    "    \n",
    "    if x_pos == 0 or y_pos == 0 or x_pos == (L-1) or y_pos == (W-1):\n",
    "        r = -100\n",
    "    elif pos == [2,2] or pos == [2,3] or pos == [2,4] or pos == [4,2] or pos == [4,3] or pos == [4,4]:\n",
    "        r = -1\n",
    "    elif pos == [3,4]:\n",
    "        r = 1\n",
    "    else: r = 0\n",
    "     \n",
    "    # print(\"reward for state (%d,%d,%d) is %d\" %(s[0],s[1],s[2],r))\n",
    "    return r\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward function setting up\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rwd_map = []\n",
    "i = 0\n",
    "\n",
    "for s in StateMatrix:\n",
    "    if not(i % 12):\n",
    "        rwd_map.append(rewardFun(s,S))\n",
    "    i = i + 1\n",
    "\n",
    "rwd_map = np.asarray(rwd_map)\n",
    "rwd = rwd_map.reshape(6,6).transpose()\n",
    "fig = plt.imshow(rwd,origin = \"lower\",vmin = -10,vmax = 1)\n",
    "print(\"Reward function setting up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyInitialize(myStates,myActions):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function sets up a shortest-path based initial policy based on state and\n",
    "                action settings. It will generate a path to get the robot closest to goal for each\n",
    "                state\n",
    "    \n",
    "    input: myStates = class of defined states, myActions = class of defined Actions\n",
    "    return: list(initial policy for each state)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    \n",
    "    \n",
    "    # initializing policy\n",
    "    p0 = {}\n",
    "    rewardState = []\n",
    "    p_init = []\n",
    "    for s in S:\n",
    "        if rewardFun(s,myStates) == 1:\n",
    "            rewardState.append(s)\n",
    "            p0[s] = (0,0)\n",
    "    \n",
    "    \n",
    "    # initialize policy based on shortest path\n",
    "    for s in S:\n",
    "        if s not in rewardState:\n",
    "            x_rwd = rewardState[0][0]\n",
    "            y_rwd = rewardState[0][1]\n",
    "            xs = np.copy(s[0])\n",
    "            ys = np.copy(s[1])\n",
    "            hs = np.copy(s[2])\n",
    "            stateMove = [np.sign(x_rwd - xs),np.sign(y_rwd - ys)]\n",
    "            if hs in np.array([11,0,1]):\n",
    "                p0[s] = (stateMove[1],stateMove[0])\n",
    "            elif hs in np.array([2,3,4]):\n",
    "                p0[s] = (stateMove[0],-stateMove[1])\n",
    "            elif hs in np.array([5,6,7]):\n",
    "                p0[s] = (-stateMove[1],stateMove[0])\n",
    "            else:\n",
    "                p0[s] = (-stateMove[0],-stateMove[1])\n",
    "            \n",
    "            # if reward lies next to the current state but not in travel direction, go backward\n",
    "            if p0[s] == (0,1) or p0[s] == (0,-1):\n",
    "                p0[s] = (-1,0)\n",
    "        \n",
    "        p_init.append(A.index(p0[s]))\n",
    "        \n",
    "    return(p_init)\n",
    "        \n",
    "\n",
    "def trajectoryFun(pe,pi,s,myStates,myActions):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function generates a trajectory to achieve goal based on input state\n",
    "                and input policy\n",
    "    \n",
    "    input: pe = error probability, pi = [policy for each state], s = current state, myState\n",
    "            = class of defined states, myActions = class of defined actions\n",
    "    return: [trajectory for input state], plot of trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    state_action = {}\n",
    "    s_traj = []\n",
    "    i = 0\n",
    "    \n",
    "    # get trajectory updated for every action done based in input state\n",
    "    while rewardFun(s,myStates) <= 0:\n",
    "        i = S.index(s)\n",
    "        state_action[s] = pi[i]\n",
    "        s_new = stateUpdate(pe,s,A[pi[i]],myStates)\n",
    "        s_traj.append(s_new)\n",
    "        s = s_new\n",
    "        \n",
    "    \n",
    "    # plot trajectory\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    i = 0\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for s in s_traj:\n",
    "        xs.append(s[0])\n",
    "        ys.append(s[1])\n",
    "        \n",
    "    plt.plot(xs,ys)\n",
    "    plt.axis([0,L-1,0,W-1])\n",
    "    plt.grid(True)\n",
    "        \n",
    "    return(s_traj)\n",
    "\n",
    "\n",
    "def valueFun(pe,pi,gamma,myStates,myActions):\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    V = {}\n",
    "    i = 0\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    for s in S:\n",
    "        V[s] = rewardFun(s,myStates)+sum(transitionProbability(pe,s,A[pi[i]],s2,myStates)*V[s2]*gamma for s2 in S)\n",
    "        i = i + 1\n",
    "        \n",
    "    return(V)\n",
    "    \n",
    "    \n",
    "        \n",
    "def policyIteration(pe,pol,gamma,myStates,myActions):\n",
    "    \"\"\"\n",
    "    Description: This function use policy iteration to get an optimal policy based on initialized policy\n",
    "    \n",
    "    input: pe = error_probability, myStates = class of defined states, myAction = class of \n",
    "            defined actions, gamma = time_discount, epsilon = convergence threshold\n",
    "    \n",
    "    return: list(policy,value)\n",
    "    \n",
    "    \"\"\"\n",
    "    start = timer()\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    V = {}\n",
    "    pi = np.copy(pol)\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    # initialize revised policy to enter in loop\n",
    "    pi_optimal = np.copy(pi)-np.copy(pi)\n",
    "    ct = 0\n",
    "    while any(pi - pi_optimal):\n",
    "        i = 0\n",
    "        pi_optimal = np.copy(pi)\n",
    "        for s in S:\n",
    "            value_action = []\n",
    "            for a in range(0,7):\n",
    "                # append value_action list for state s for all actions\n",
    "                value_action.append(rewardFun(s,myStates)+gamma*sum(\n",
    "                    transitionProbability(pe,s,A[a],s2,myStates)*V[s2] for s2 in S))\n",
    "                    \n",
    "            # set policy with highest value\n",
    "            pi[i] = np.argmax(value_action)\n",
    "            V[s] = max(value_action)\n",
    "            i = i + 1\n",
    "      \n",
    "        ct = ct + 1\n",
    "        print(\"policy iterates for %d times......\" %(ct))\n",
    "        \n",
    "    end = timer()\n",
    "    \n",
    "    print(\"policy iteration done in %f seconds\" %(end-start))\n",
    "        \n",
    "    return(pi_optimal,V)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial policy setting up with action ID...\n",
      "[2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 6, 6, 5, 5, 5, 3, 3, 3, 2, 2, 2, 6, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 6, 6, 5, 5, 5, 3, 3, 3, 2, 2, 2, 6]\n",
      "trajectory for robot using initialized policy: \n",
      "[(1, 5, 6), (1, 4, 7), (1, 5, 7), (1, 4, 8), (2, 4, 8), (3, 4, 8)]\n",
      "values along initialized trajectory: -100.000000,0.000000,-100.000000,0.000000,-1.000000,1.000000\n",
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iteration done in 67.160579 seconds\n",
      "optimized policy is: \n",
      "[5 2 3 2 2 6 3 5 6 5 5 3 2 2 1 1 1 6 5 5 4 4 4 3 2 2 3 1 2 6 5 5 6 4 5 3 2\n",
      " 2 3 1 2 6 2 5 6 4 5 3 2 5 1 1 1 3 2 2 4 4 4 6 2 5 3 2 2 3 6 2 6 5 5 6 1 2\n",
      " 3 2 2 6 4 5 6 5 5 3 1 1 1 2 1 4 4 4 4 5 4 1 2 1 1 2 1 4 5 4 4 5 4 1 1 2 1\n",
      " 2 1 6 1 5 4 5 4 3 5 4 1 1 1 1 2 1 4 4 4 4 4 5 3 2 2 3 1 2 6 5 5 6 2 2 3 2\n",
      " 2 6 5 5 6 2 5 3 2 2 3 1 2 6 5 5 6 1 5 3 2 2 3 1 2 6 5 5 6 1 5 3 2 2 3 1 2\n",
      " 6 2 5 6 1 5 3 5 5 1 1 1 3 2 2 4 4 4 6 5 5 3 2 2 3 2 2 6 2 5 6 1 1 3 2 2 4\n",
      " 4 4 3 2 2 1 1 1 1 2 1 4 4 4 1 2 1 1 1 1 1 2 1 4 4 4 1 2 1 1 1 1 1 2 1 4 4\n",
      " 4 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 4 4 3 2 2 1 1 1 3 2 2 4 2 2 6 2 5 6 5 5\n",
      " 3 2 2 3 2 2 6 4 5 6 5 5 3 1 2 3 2 2 6 4 5 6 5 5 3 1 2 3 2 2 6 4 5 6 2 5 3\n",
      " 1 2 3 5 5 4 4 4 3 2 2 1 1 1 6 5 5 6 2 5 3 2 2 3 2 2 6 5 2 6 5 5 6 3 5 3 2\n",
      " 2 3 2 2 4 5 4 6 5 5 1 2 1 3 2 2 4 5 4 6 5 5 1 2 1 3 2 2 4 5 4 6 2 5 1 2 1\n",
      " 3 2 5 4 4 4 3 2 2 1 1 1 6 2 5 6 5 5 3 6 2 3 2 2 6]\n",
      "trajectory for optimized policy: \n",
      "[(1, 3, 7), (1, 4, 8), (2, 4, 8), (3, 4, 8)]\n",
      "values for optimized policy: \n",
      "2.9932155990000004\n",
      "3.713215599\n",
      "4.5132155990000005\n",
      "6.5132155990000005\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialized policy\n",
    "p_init = policyInitialize(S,A)\n",
    "print(\"initial policy setting up with action ID...\")\n",
    "print(p_init)\n",
    "print(\"trajectory for robot using initialized policy: \")\n",
    "s_traj = trajectoryFun(0,p_init,(1,4,6),S,A)\n",
    "print(s_traj)\n",
    "V = valueFun(0,p_init,0.9,S,A)\n",
    "print(\"values along initialized trajectory: %f,%f,%f,%f,%f,%f\" \n",
    "      %(V[(1,5,6)],V[(1,4,7)],V[(1,5,7)],V[(1,4,8)],V[(2,4,8)],V[(3,4,8)]))\n",
    "\n",
    "# Optimized policy and value from policy iteration\n",
    "pi_optimal,v_optimal = policyIteration(0,p_init,0.9,S,A)\n",
    "print(\"optimized policy is: \")\n",
    "print(pi_optimal)\n",
    "\n",
    "\n",
    "# Optimized Trajectory\n",
    "s_traj_opt = trajectoryFun(0,pi_optimal,(1,4,6),S,A)\n",
    "print(\"trajectory for optimized policy: \")\n",
    "print(s_traj_opt)\n",
    "print(\"values for optimized policy: \")\n",
    "for s in v_optimal.keys():\n",
    "    if s in s_traj_opt:\n",
    "        print(v_optimal[s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(pe,myStates,myActions,gamma,epsilon=0.5):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function takes error probability, action, current state,\n",
    "                next state, state map and time discount as input and returns the optimal \n",
    "                policy for each state as well as value for each state\n",
    "    \n",
    "    input: pe = error_probability, myStates = class of defined states, myAction = class of \n",
    "            defined actions, gamma = time_discount, epsilon = convergence threshold\n",
    "    return: list(policy,value)\n",
    "    \n",
    "    NOTE: 1.This function runs slowly as not optimized for efficiency, please be patient\n",
    "          2. epsilon is usually set to be 0.01 and default value 0.5 used for demo\n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    if epsilon > 0.5 or epsilon < 0.0:\n",
    "        raise Exception(\"Invalid epsilon. Enter a value between 0 and 0.5\")\n",
    "        \n",
    "        \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    # initializae all states' values\n",
    "    V = {}\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    pi = {}\n",
    "    max_value = {}\n",
    "        \n",
    "        \n",
    "    # set a large threshold bigger than epsilon to enter in the loop\n",
    "    delta = 10000\n",
    "    \n",
    "    # perform value iteration\n",
    "    while delta > epsilon:\n",
    "        \n",
    "        # initialize delta to be zero, construct empty list for policy and value\n",
    "        delta = 0\n",
    "        policy = []\n",
    "        value = []\n",
    "        \n",
    "        # iteration for agent being at state s in S\n",
    "        for s in S:\n",
    "            v = np.copy(V[s])\n",
    "            value_action = []\n",
    "            \n",
    "            # for all actions, find value for action a at state s, sum over all future state s2\n",
    "            for a in range(0,7):\n",
    "                value_action.append(sum(transitionProbability(pe,s,A[a],s2,myStates)*(rewardFun(s2,myStates)+gamma*V[s2]) for s2 in S))\n",
    "            V[s] = max(value_action)\n",
    "            pi[s] = np.argmax(value_action)\n",
    "            \n",
    "            # construct policy list and value list\n",
    "            policy.append(pi[s])\n",
    "            value.append(V[s])\n",
    "            \n",
    "            # update delta for each state\n",
    "            delta = max(delta,abs(v-V[s]))\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Value iteration costs %f seconds.......\" %(end - start))\n",
    "\n",
    "    return(policy,value)\n",
    "\n",
    "\n",
    "def mappingState(policy,value,myStates,myActions):\n",
    "    A = myActions.Set\n",
    "    S = myStates.stateMatrix\n",
    "    \n",
    "    # convert input list to array\n",
    "    V = np.asarray(value)\n",
    "    P = np.asarray(policy)\n",
    "    \n",
    "    vmap = {}\n",
    "    pmap = {}\n",
    "    \n",
    "    \n",
    "    # mapping state with policy and value\n",
    "    ct = 0\n",
    "    for s in S:\n",
    "        vmap[s] = V[ct]\n",
    "        pmap[s] = A[P[ct]]\n",
    "        ct = ct + 1\n",
    "        \n",
    "    return(vmap,pmap)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration costs 63.326981 seconds.......\n",
      "optimal policy using value iteration is: \n",
      "[5, 2, 3, 2, 2, 6, 3, 5, 6, 5, 5, 3, 2, 2, 1, 1, 1, 6, 5, 5, 4, 4, 4, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 4, 5, 3, 2, 2, 3, 1, 2, 6, 2, 5, 6, 4, 5, 3, 2, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 2, 5, 3, 2, 2, 3, 6, 2, 6, 5, 5, 6, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 1, 1, 2, 1, 4, 4, 4, 4, 5, 4, 1, 2, 1, 1, 2, 1, 4, 5, 4, 4, 5, 4, 1, 1, 2, 1, 2, 1, 6, 1, 5, 4, 5, 4, 3, 5, 4, 1, 1, 1, 1, 2, 1, 4, 4, 4, 4, 4, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 2, 2, 3, 2, 2, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 1, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 1, 5, 3, 2, 2, 3, 1, 2, 6, 2, 5, 6, 1, 5, 3, 5, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 2, 5, 6, 1, 1, 3, 2, 2, 4, 4, 4, 3, 2, 2, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 2, 2, 1, 1, 1, 3, 2, 2, 4, 2, 2, 6, 2, 5, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 2, 5, 3, 1, 2, 3, 5, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 2, 2, 6, 5, 2, 6, 5, 5, 6, 3, 5, 3, 2, 2, 3, 2, 2, 4, 5, 4, 6, 5, 5, 1, 2, 1, 3, 2, 2, 4, 5, 4, 6, 5, 5, 1, 2, 1, 3, 2, 2, 4, 5, 4, 6, 2, 5, 1, 2, 1, 3, 2, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 2, 5, 6, 5, 5, 3, 6, 2, 3, 2, 2, 6]\n",
      "comparison with policy iteration: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_optimal_v,v_optimal = ValueIteration(0,S,A,0.9)\n",
    "print(\"optimal policy using value iteration is: \")\n",
    "print(pi_optimal_v)\n",
    "\n",
    "print(\"comparison with policy iteration: \")\n",
    "print(pi_optimal_v - pi_optimal)\n",
    "s_traj_v = trajectoryFun(0,pi_optimal_v,(1,4,6),S,A)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2, 3, 2, 2, 6, 3, 5, 6, 5, 5, 3, 2, 2, 1, 1, 1, 6, 5, 5, 4, 4, 4, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 4, 5, 3, 2, 2, 3, 1, 2, 6, 2, 5, 6, 4, 5, 3, 2, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 2, 5, 3, 2, 2, 3, 6, 2, 6, 5, 5, 6, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 1, 1, 2, 1, 4, 4, 4, 4, 5, 4, 1, 2, 1, 1, 2, 1, 4, 5, 4, 4, 5, 4, 1, 1, 2, 1, 2, 1, 6, 1, 5, 4, 5, 4, 3, 5, 4, 1, 1, 1, 1, 2, 1, 4, 4, 4, 4, 4, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 2, 2, 3, 2, 2, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 1, 5, 3, 2, 2, 3, 1, 2, 6, 5, 5, 6, 1, 5, 3, 2, 2, 3, 1, 2, 6, 2, 5, 6, 1, 5, 3, 5, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 2, 5, 6, 1, 1, 3, 2, 2, 4, 4, 4, 3, 2, 2, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 2, 2, 1, 1, 1, 3, 2, 2, 4, 2, 2, 6, 2, 5, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 2, 5, 3, 1, 2, 3, 5, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 2, 2, 6, 5, 2, 6, 5, 5, 6, 3, 5, 3, 2, 2, 3, 2, 2, 4, 5, 4, 6, 5, 5, 1, 2, 1, 3, 2, 2, 4, 5, 4, 6, 5, 5, 1, 2, 1, 3, 2, 2, 4, 5, 4, 6, 2, 5, 1, 2, 1, 3, 2, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 2, 5, 6, 5, 5, 3, 6, 2, 3, 2, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "vmap,pmap = mappingState(policy,value,S,A)\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
