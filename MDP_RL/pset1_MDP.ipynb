{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup\n",
    "Consider a simple robot in a 2D grid world of length L and width W. That is, the robot can be located at any lattice\n",
    "point (x, y) : 0 ≤ x < L, 0 ≤ y < W; x, y ∈ N. At each point, the robot can face any of the twelve headings identified\n",
    "by the hours on a clock h ∈ {0 . . . 11}, where h = 0 represents 12 o’clock is pointing up (i.e. positive y) and h = 3 is\n",
    "pointing to the right (i.e. positive x).\n",
    "In each time step, the robot can chose from several actions. Each singular action will consist of a movement\n",
    "followed by a rotation.<br/>\n",
    "• The robot can choose to take no motion at all, staying still and neither moving nor rotating.<br/>\n",
    "• Otherwise the robot can choose to either move “forwards” or “backwards”.<br/>\n",
    "– This may cause a pre-rotation error, see below.<br/>\n",
    "– This will cause the robot move one unit in the direction it is facing, rounded to the nearest cardinal\n",
    "direction.<br/>\n",
    "That is, if the robot is pointing towards either 2, 3, or 4 and opts to move “forwards”, the robot will move\n",
    "one unit in the +x direction. Similarly, if the robot is pointing towards either 11, 0, or 1 and opts to move\n",
    "“backwards”, it will move one unit in the −y direction.<br/>\n",
    "• After the movement, the robot can choose to turn left, not turn, or turn right. A left (counter-clockwise) turn\n",
    "will decrease the heading by 1 (mod 12); right (clockwise) will increase the heading by 1 (mod 12). The robot\n",
    "can also keep the heading constant.<br/>\n",
    "• Attempting to move off of the grid will result in no linear movement, but the rotation portion of the action\n",
    "will still happen.<br/>\n",
    "\n",
    "Note that aside from the at edges of the grids, the robot can only rotate if it also moves forwards or backwards; it\n",
    "can move without rotating though.\n",
    "The robot has an error probability pe: if the robot choses to move, it will first rotate by +1 or -1 (mod 12) with\n",
    "probability pe each, before it moves. It will not pre-rotate with probability 1 − 2pe. Choosing to stay still (take no\n",
    "motion) will not incur an error rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class myStates:\n",
    "    def __init__(self,Length,Width):\n",
    "        print(\"new state created with size %d X %d X 12...\" % (Length,Width))\n",
    "        self.L = Length\n",
    "        self.W = Width\n",
    "        self.sz = Length*Width*12\n",
    "        if self.L <= 0 or self.W <= 0:\n",
    "            raise Exception('Dimension of state should be positive integers')\n",
    "        \n",
    "        self.stateMatrix = []\n",
    "        # Create state list\n",
    "        dir_mat = np.array(range(12))\n",
    "        for i in range(self.L):\n",
    "            for j in range(self.W):\n",
    "                for k in dir_mat:\n",
    "                    self.stateMatrix.append((i,j,k))\n",
    "                    \n",
    "# Creating Actions\n",
    "class myActions:\n",
    "    def __init__(self,act = None,turn = None):\n",
    "        self.actionMat = (act,turn)\n",
    "        self.sz = 7\n",
    "        self.Set = list()\n",
    "        self.Set.append((0,0))\n",
    "        self.Set.append((1,0))\n",
    "        self.Set.append((1,1))\n",
    "        self.Set.append((1,-1))\n",
    "        self.Set.append((-1,0))\n",
    "        self.Set.append((-1,1))\n",
    "        self.Set.append((-1,-1))\n",
    "        # 0: (0,0): Stay still\n",
    "        # 1: (1,0): Forward only\n",
    "        # 2: (1,1): Forward clockwise\n",
    "        # 3: (1,-1): Forward counter-clockwise\n",
    "        # 4: (-1,0): Backward only\n",
    "        # 5: (-1,1): Backward clockwise\n",
    "        # 6: (-1,-1): Backward counter-clockwise\n",
    "        print('action setting up done...')\n",
    "        \n",
    "# Creating Probability Space functions\n",
    "def transitionProbability(pe,s,a,s_next,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: this function takes error probability pe, current state s, action and future\n",
    "                state as inputs, returns the transition probability between each state\n",
    "                \n",
    "    Input: pe = error probability, s = (x,y,h), a = (heading,rotation), s_next = (x',y',h'), \n",
    "            myStates = class of defined states\n",
    "            \n",
    "    Output: p = probability \n",
    "\n",
    "    \"\"\"\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    # pe threshold\n",
    "    if pe > 0.5 or pe < 0.0:\n",
    "        raise Exception('Error probability should lie between 0 and 0.5')\n",
    "    \n",
    "    # define possible cartesian movement\n",
    "    pos_x = [1,0]\n",
    "    pos_y = [0,1]\n",
    "    neg_x = [-1,0]\n",
    "    neg_y = [0,-1]\n",
    "    \n",
    "    # create a dictionary for possible heading direction based on current heading,\n",
    "    # consisting of three possible heading configuration for next state\n",
    "    \n",
    "    # h_dic[h] = [(moving_direction,h',possibility),~,~]\n",
    "    h_dic = {}\n",
    "    \n",
    "    h_dic[0] = [(pos_y,0,1-2*pe),(pos_y,1,pe),(pos_x,11,pe)]\n",
    "    h_dic[1] = [(pos_y,1,1-2*pe),(pos_x,2,pe),(pos_x,0,pe)]\n",
    "    h_dic[2] = [(pos_x,2,1-2*pe),(pos_x,3,pe),(pos_x,1,pe)]\n",
    "    h_dic[3] = [(pos_x,3,1-2*pe),(pos_x,4,pe),(neg_y,2,pe)]\n",
    "    h_dic[4] = [(pos_x,4,1-2*pe),(neg_y,5,pe),(neg_y,3,pe)]\n",
    "    h_dic[5] = [(neg_y,5,1-2*pe),(neg_y,6,pe),(neg_y,4,pe)]\n",
    "    h_dic[6] = [(neg_y,6,1-2*pe),(neg_y,7,pe),(neg_x,5,pe)]\n",
    "    h_dic[7] = [(neg_y,7,1-2*pe),(neg_x,8,pe),(neg_x,6,pe)]\n",
    "    h_dic[8] = [(neg_x,8,1-2*pe),(neg_x,9,pe),(neg_x,7,pe)]\n",
    "    h_dic[9] = [(neg_x,9,1-2*pe),(neg_x,10,pe),(pos_y,8,pe)]\n",
    "    h_dic[10] = [(neg_x,10,1-2*pe),(pos_y,11,pe),(pos_y,9,pe)]\n",
    "    h_dic[11] = [(pos_y,11,1-2*pe),(pos_y,0,pe),(pos_y,10,pe)]\n",
    "    \n",
    "    \n",
    "    # create a dictionary for transition probability based on future state \n",
    "    # and current state\n",
    "    transProb = {}\n",
    "\n",
    "    for map_key in h_dic[s[2]]:\n",
    "        x_new = s[0] + a[0]*map_key[0][0]   # move in x direction, a[0] indicates forward or backward\n",
    "        xd = x_new if (x_new <= L-1 and x_new >= 0) else s[0]       # else for off-grid movement\n",
    "        y_new = s[1] + a[0]*map_key[0][1]   # move in y direction, a[0] indicates forward or backward\n",
    "        yd = y_new if (y_new <= W-1 and y_new >= 0) else s[1]       # else for off-grid movement\n",
    "        hd = (map_key[1] + a[1]) % 12       # new heading direction\n",
    "        if a[0] == 0 and a[1] == 0:\n",
    "            transProb[s] = 1\n",
    "        else: transProb[(xd,yd,hd)] = map_key[2]\n",
    "    \n",
    "    # match with the keys in transProb dictionary\n",
    "    if s_next in transProb.keys():\n",
    "       # print(\"p = %f\" %(transProb[s_next]))\n",
    "        return(transProb[s_next]);\n",
    "    else: \n",
    "       # print(\"p = 0\")\n",
    "        return 0.0\n",
    "    \n",
    "        \n",
    "\n",
    "# update state based on action and current state\n",
    "def stateUpdate(pe,s,a,myStates):\n",
    "    \"\"\"\n",
    "    Description: This function performs updating of current state s based on action it takes and error\n",
    "                probability when doing action\n",
    "                \n",
    "    Input: pe = error probability, s = (x,y,h), a = (heading,direction), myStates = class of defined states\n",
    "    \n",
    "    Output: s_next = (x',y',h')\n",
    "    \"\"\"\n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    P = []\n",
    "    # search for probability trasferring to state s_next given current state and action\n",
    "    for s_next in S:\n",
    "        pt = transitionProbability(pe,s,a,s_next,myStates)\n",
    "        if pt != 0:\n",
    "            P.append((s_next,pt))\n",
    "    \n",
    "    prob = np.array([])\n",
    "    for p in P:\n",
    "        prob = np.append(prob,p[1])\n",
    "\n",
    "    # return a choice given discrete pdf\n",
    "    state_id = np.random.choice(np.arange(len(P)),p=prob)\n",
    "    s_next = P[state_id][0]\n",
    "    return(s_next)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new state created with size 6 X 6 X 12...\n",
      "action setting up done...\n",
      "The size of {S} Ns is L X W X 12, for a 6 X 6 grid, Ns is 432\n",
      "The size of {A} Na is 7\n"
     ]
    }
   ],
   "source": [
    "S = myStates(6,6)\n",
    "A = myActions()\n",
    "StateMatrix = S.stateMatrix\n",
    "ActionSet = A.Set\n",
    "\n",
    "print(\"The size of {S} Ns is L X W X 12, for a 6 X 6 grid, Ns is %d\" %S.sz)\n",
    "print(\"The size of {A} Na is %d\" %A.sz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "The rewards for each state are independent of heading angle (or action taken). The border states {x = 0, x =\n",
    "L, y = 0, y = W} have reward -100 (purple in map). The lane markers (cyan in map) have reward -10. The\n",
    "goal square (yellow in map) has reward +1. Every other state has reward 0 (green in map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reward map for state input\n",
    "def rewardFun(s,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: This function takes a certain state in state map and output the reward at that state\n",
    "    \n",
    "    Input: s = current state, myStates = class of defined states\n",
    "    \n",
    "    Return: float(reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from states\n",
    "    S = myStates.stateMatrix\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    \n",
    "    \n",
    "    x_pos = s[0]\n",
    "    y_pos = s[1]\n",
    "    h = s[2]\n",
    "    \n",
    "    if x_pos < 0 or x_pos >= L or y_pos < 0 or y_pos >= W or h < 0 or h >= 12:\n",
    "        raise Exception('Invalid state definition: [x,y,h] should be within range')\n",
    "    \n",
    "    pos = [x_pos,y_pos]\n",
    "    \n",
    "    if x_pos == 0 or y_pos == 0 or x_pos == (L-1) or y_pos == (W-1):\n",
    "        r = -100\n",
    "    elif pos == [2,2] or pos == [2,3] or pos == [2,4] or pos == [4,2] or pos == [4,3] or pos == [4,4]:\n",
    "        r = -10\n",
    "    elif pos == [3,4]:\n",
    "        r = 1\n",
    "    else: r = 0\n",
    "     \n",
    "    # print(\"reward for state (%d,%d,%d) is %d\" %(s[0],s[1],s[2],r))\n",
    "    return r\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding map for reward function is illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rwd_map = []\n",
    "i = 0\n",
    "\n",
    "for s in StateMatrix:\n",
    "    if not(i % 12):\n",
    "        rwd_map.append(rewardFun(s,S))\n",
    "    i = i + 1\n",
    "\n",
    "rwd_map = np.asarray(rwd_map)\n",
    "rwd = rwd_map.reshape(6,6).transpose()\n",
    "fig = plt.imshow(rwd,origin = \"lower\",vmin = -15,vmax = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "Assume an initial policy π0 of taking the action that gets you closest to the goal square. That is, if the goal is in\n",
    "front of you, move forward; if it is behind you, move backwards; then turn the amount that aligns your next direction\n",
    "of travel closer towards the goal (if necessary). If the goal is directly to your left or right, move forward then turn\n",
    "appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyInitialize(myStates,myActions):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function sets up a shortest-path based initial policy based on state and\n",
    "                action settings. It will generate a path to get the robot closest to goal for each\n",
    "                state. It only cares about the goal and it ignores all penalties (negative rewards)\n",
    "                in the map\n",
    "    \n",
    "    input: myStates = class of defined states, myActions = class of defined Actions\n",
    "    return: list(initial policy for each state)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    \n",
    "    \n",
    "    # initializing policy\n",
    "    p0 = {}\n",
    "    rewardState = []\n",
    "    p_init = []\n",
    "    for s in S:\n",
    "        if rewardFun(s,myStates) == 1:\n",
    "            rewardState.append(s)\n",
    "            p0[s] = (0,0)\n",
    "    \n",
    "    \n",
    "    # initialize policy based on shortest path\n",
    "    for s in S:\n",
    "        if s not in rewardState:\n",
    "            x_rwd = rewardState[0][0]\n",
    "            y_rwd = rewardState[0][1]\n",
    "            xs = np.copy(s[0])\n",
    "            ys = np.copy(s[1])\n",
    "            hs = np.copy(s[2])\n",
    "            stateMove = [np.sign(x_rwd - xs),np.sign(y_rwd - ys)]\n",
    "            if hs in np.array([11,0,1]):\n",
    "                p0[s] = (stateMove[1],stateMove[0])\n",
    "            elif hs in np.array([2,3,4]):\n",
    "                p0[s] = (stateMove[0],-stateMove[1])\n",
    "            elif hs in np.array([5,6,7]):\n",
    "                p0[s] = (-stateMove[1],stateMove[0])\n",
    "            else:\n",
    "                p0[s] = (-stateMove[0],-stateMove[1])\n",
    "            \n",
    "            # if reward lies next to the current state but not in travel direction, go backward\n",
    "            if p0[s] == (0,1) or p0[s] == (0,-1):\n",
    "                p0[s] = (-1,0)\n",
    "        \n",
    "        p_init.append(A.index(p0[s]))\n",
    "        \n",
    "    return(p_init)\n",
    "        \n",
    "\n",
    "def trajectoryFun(pe,pi,s,myStates,myActions):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function generates a trajectory to achieve goal based on input state\n",
    "                and input policy\n",
    "    \n",
    "    input: pe = error probability, pi = [policy for each state], s = current state, myState\n",
    "            = class of defined states, myActions = class of defined actions\n",
    "    return: [trajectory for input state], plot of trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    state_action = {}\n",
    "    s_traj = []\n",
    "    i = 0\n",
    "    \n",
    "    # get trajectory updated for every action done based in input state\n",
    "    while rewardFun(s,myStates) <= 0:\n",
    "        i = S.index(s)\n",
    "        state_action[s] = pi[i]\n",
    "        s_new = stateUpdate(pe,s,A[pi[i]],myStates)\n",
    "        s_traj.append(s_new)\n",
    "        s = s_new\n",
    "        \n",
    "    \n",
    "    # plot trajectory\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    i = 0\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for s in s_traj:\n",
    "        xs.append(s[0])\n",
    "        ys.append(s[1])\n",
    "        \n",
    "    plt.plot(xs,ys)\n",
    "    plt.axis([0,L-1,0,W-1])\n",
    "    plt.grid(True)\n",
    "        \n",
    "    return(s_traj)\n",
    "\n",
    "\n",
    "def valueFun(pe,pi,gamma,myStates,myActions):\n",
    "    \"\"\"\n",
    "    Description: This function calculates value of each state for a given policy and discounts\n",
    "    \n",
    "    Input: pe = error probability, pi = list(given policy for all states), gamma = discounts\n",
    "    \n",
    "    Output: list(values for each state)\n",
    "    \"\"\"\n",
    "    if gamma > 1 or gamma < 0:\n",
    "        raise Exception(\"Invalid input. gamma should be in range[0,1])\")\n",
    "    \n",
    "    \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    V = {}\n",
    "    i = 0\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    for s in S:\n",
    "        V[s] = rewardFun(s,myStates)+sum(transitionProbability(pe,s,A[pi[i]],s2,myStates)*V[s2]*gamma for s2 in S)\n",
    "        i = i + 1\n",
    "        \n",
    "    return(V)\n",
    "    \n",
    "    \n",
    "        \n",
    "def policyIteration(pe,pol,gamma,myStates,myActions):\n",
    "    \"\"\"\n",
    "    Description: This function use policy iteration to get an optimal policy based on initialized policy\n",
    "    \n",
    "    input: pe = error_probability, myStates = class of defined states, myAction = class of \n",
    "            defined actions, gamma = time_discount, epsilon = convergence threshold\n",
    "    \n",
    "    return: list(policy,value)\n",
    "    \n",
    "    Note: This function runs slowly. Please be patient\n",
    "    \"\"\"\n",
    "    if gamma > 1 or gamma < 0:\n",
    "        raise Exception(\"Invalid input. gamma should be in range[0,1])\")\n",
    "    \n",
    "    \n",
    "    start = timer()\n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    V = {}\n",
    "    pi = np.copy(pol)\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    # initialize revised policy to enter in loop\n",
    "    pi_optimal = np.copy(pi)-np.copy(pi)\n",
    "    ct = 0\n",
    "    while any(pi - pi_optimal):\n",
    "        i = 0\n",
    "        pi_optimal = np.copy(pi)\n",
    "        for s in S:\n",
    "            value_action = []\n",
    "            for a in range(0,7):\n",
    "                # append value_action list for state s for all actions\n",
    "                value_action.append(rewardFun(s,myStates)+gamma*sum(\n",
    "                    transitionProbability(pe,s,A[a],s2,myStates)*V[s2] for s2 in S))\n",
    "                    \n",
    "            # set policy with highest value\n",
    "            pi[i] = np.argmax(value_action)\n",
    "            V[s] = max(value_action)\n",
    "            i = i + 1\n",
    "      \n",
    "        ct = ct + 1\n",
    "        print(\"policy iterates for %d times......\" %(ct))\n",
    "        \n",
    "    end = timer()\n",
    "    \n",
    "    print(\"policy iteration done in %f seconds\" %(end-start))\n",
    "        \n",
    "    return(pi_optimal,V)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectory for robot is plotted below on state map. Blue route is based on initialized policy. It doesn't care about lane segments and cliff (-10 and -100 on reward map) while it only cares about reward  (+1 on reward map). Orange route is based on optmized policy through policy iteration. It avoides all lanes and reach the goal as expected. Note that the route is largely affected by pe (error probability) and reward function, as well as time discount. If lane penalty is not large enough (i.e. -1), the robot will short-cut thorugh the lane to get to the goal state. For the whole policy iteration, it takes approximately 65 seconds to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial policy setting up with action ID...\n",
      "[2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 2, 2, 3, 3, 3, 5, 5, 5, 6, 6, 6, 2, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 4, 4, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 6, 6, 5, 5, 5, 3, 3, 3, 2, 2, 2, 6, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 6, 6, 5, 5, 5, 3, 3, 3, 2, 2, 2, 6]\n",
      "trajectory for robot using initialized policy: \n",
      "[(1, 5, 6), (1, 4, 7), (1, 5, 7), (1, 4, 8), (2, 4, 8), (3, 4, 8)]\n",
      "values along initialized trajectory: -100.000000,0.000000,-100.000000,0.000000,-10.000000,1.000000\n",
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iteration done in 63.246273 seconds\n",
      "optimized policy is: \n",
      "[5 2 3 2 2 6 3 5 6 6 5 3 2 2 2 1 1 6 2 5 4 4 6 3 2 5 3 1 2 3 2 2 6 4 5 6 5\n",
      " 5 3 1 2 3 2 2 6 4 5 6 5 5 3 1 2 3 2 2 6 4 5 6 5 5 3 5 2 3 2 2 6 3 5 6 1 2\n",
      " 3 2 2 6 4 5 6 6 5 3 2 1 1 2 1 4 5 4 4 6 4 1 1 5 0 0 0 3 1 2 0 0 0 6 5 4 0\n",
      " 0 0 1 2 1 0 0 0 4 4 4 0 0 0 1 1 1 0 0 0 4 4 4 3 2 2 1 1 1 6 6 5 4 1 2 3 2\n",
      " 2 6 4 5 6 3 2 3 0 0 2 1 2 0 0 0 6 1 3 0 4 5 3 1 2 3 1 2 6 1 2 6 5 4 3 1 2\n",
      " 3 2 5 6 1 2 4 4 5 3 1 2 3 2 2 4 6 2 6 5 5 3 2 2 3 2 2 6 3 2 6 1 2 3 2 2 4\n",
      " 4 4 3 3 2 3 2 1 1 2 1 4 4 4 1 3 1 1 1 5 0 0 0 4 4 4 0 0 0 6 5 4 0 0 0 5 4\n",
      " 4 0 0 0 4 4 4 0 0 0 1 0 0 0 0 0 4 4 4 3 2 2 2 1 1 3 3 2 4 1 2 6 2 5 6 4 5\n",
      " 3 3 2 3 0 0 5 4 5 0 0 0 3 1 3 0 4 5 6 4 5 3 1 2 3 1 2 6 5 4 6 4 5 3 2 5 3\n",
      " 1 2 4 4 5 6 4 5 3 2 2 1 3 2 6 5 5 6 2 5 3 2 2 3 3 2 6 5 2 6 5 5 6 3 5 3 3\n",
      " 2 3 2 2 4 5 4 6 2 5 1 3 1 3 3 5 6 5 4 3 5 2 1 3 2 6 5 5 4 5 4 3 2 5 1 3 1\n",
      " 3 3 5 4 5 4 3 2 2 1 1 3 6 3 5 6 5 5 3 5 2 3 3 2 6]\n",
      "trajectory for optimized policy: \n",
      "[(1, 3, 6), (1, 2, 7), (1, 1, 8), (2, 1, 8), (3, 1, 7), (3, 2, 7), (3, 3, 7), (3, 4, 7)]\n",
      "values for optimized policy: \n",
      "2.4181155990000005\n",
      "2.1763040391000006\n",
      "1.9586736351900005\n",
      "3.074215599000001\n",
      "3.803215599000001\n",
      "4.613215599\n",
      "5.5132155990000005\n",
      "6.5132155990000005\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialized policy\n",
    "p_init = policyInitialize(S,A)\n",
    "print(\"initial policy setting up with action ID...\")\n",
    "print(p_init)\n",
    "print(\"trajectory for robot using initialized policy: \")\n",
    "s_traj = trajectoryFun(0,p_init,(1,4,6),S,A)\n",
    "print(s_traj)\n",
    "V = valueFun(0,p_init,0.9,S,A)\n",
    "print(\"values along initialized trajectory: %f,%f,%f,%f,%f,%f\" \n",
    "      %(V[(1,5,6)],V[(1,4,7)],V[(1,5,7)],V[(1,4,8)],V[(2,4,8)],V[(3,4,8)]))\n",
    "\n",
    "# Optimized policy and value from policy iteration\n",
    "pi_optimal,v_optimal = policyIteration(0,p_init,0.9,S,A)\n",
    "print(\"optimized policy is: \")\n",
    "print(pi_optimal)\n",
    "\n",
    "\n",
    "# Optimized Trajectory\n",
    "s_traj_opt = trajectoryFun(0,pi_optimal,(1,4,6),S,A)\n",
    "print(\"trajectory for optimized policy: \")\n",
    "print(s_traj_opt)\n",
    "print(\"values for optimized policy: \")\n",
    "for s in v_optimal.keys():\n",
    "    if s in s_traj_opt:\n",
    "        print(v_optimal[s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(pe,myStates,myActions,gamma,epsilon=0.5):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Description: this function takes error probability, action, current state,\n",
    "                next state, state map and time discount as input and returns the optimal \n",
    "                policy for each state as well as value for each state\n",
    "    \n",
    "    input: pe = error_probability, myStates = class of defined states, myAction = class of \n",
    "            defined actions, gamma = time_discount, epsilon = convergence threshold\n",
    "    return: list(policy,value)\n",
    "    \n",
    "    NOTE: 1.This function runs slowly as not optimized for efficiency, please be patient\n",
    "          2. epsilon is usually set to be 0.01 and default value 0.5 used for demo\n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    if epsilon > 0.5 or epsilon < 0.0:\n",
    "        raise Exception(\"Invalid epsilon. Enter a value between 0 and 0.5\")\n",
    "        \n",
    "        \n",
    "    S = myStates.stateMatrix\n",
    "    A = myActions.Set\n",
    "    # initializae all states' values\n",
    "    V = {}\n",
    "    for s in S:\n",
    "        V[s] = 0\n",
    "    \n",
    "    pi = {}\n",
    "    max_value = {}\n",
    "        \n",
    "        \n",
    "    # set a large threshold bigger than epsilon to enter in the loop\n",
    "    delta = 10000\n",
    "    \n",
    "    # perform value iteration\n",
    "    while delta > epsilon:\n",
    "        \n",
    "        # initialize delta to be zero, construct empty list for policy and value\n",
    "        delta = 0\n",
    "        policy = []\n",
    "        value = []\n",
    "        \n",
    "        # iteration for agent being at state s in S\n",
    "        for s in S:\n",
    "            v = np.copy(V[s])\n",
    "            value_action = []\n",
    "            \n",
    "            # for all actions, find value for action a at state s, sum over all future state s2\n",
    "            for a in range(0,7):\n",
    "                value_action.append(sum(transitionProbability(pe,s,A[a],s2,myStates)*(rewardFun(s2,myStates)+gamma*V[s2]) for s2 in S))\n",
    "            V[s] = max(value_action)\n",
    "            pi[s] = np.argmax(value_action)\n",
    "            \n",
    "            # construct policy list and value list\n",
    "            policy.append(pi[s])\n",
    "            value.append(V[s])\n",
    "            \n",
    "            # update delta for each state\n",
    "            delta = max(delta,abs(v-V[s]))\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Value iteration costs %f seconds.......\" %(end - start))\n",
    "\n",
    "    return(policy,value)\n",
    "\n",
    "\n",
    "def mappingState(policy,value,myStates,myActions):\n",
    "    A = myActions.Set\n",
    "    S = myStates.stateMatrix\n",
    "    \n",
    "    # convert input list to array\n",
    "    V = np.asarray(value)\n",
    "    P = np.asarray(policy)\n",
    "    \n",
    "    vmap = {}\n",
    "    pmap = {}\n",
    "    \n",
    "    \n",
    "    # mapping state with policy and value\n",
    "    ct = 0\n",
    "    for s in S:\n",
    "        vmap[s] = V[ct]\n",
    "        pmap[s] = A[P[ct]]\n",
    "        ct = ct + 1\n",
    "        \n",
    "    return(vmap,pmap)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part generates an optimized trajectory based on value iteration process. Compared with policy iteration process, they all converges to the same policy and same value. In addition, the run time for value iteration has no big difference as policy iteration. They all spent 65 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration costs 65.075177 seconds.......\n",
      "optimal policy using value iteration is: \n",
      "[5, 2, 3, 2, 2, 6, 3, 5, 6, 5, 5, 3, 2, 2, 1, 1, 1, 6, 2, 5, 4, 4, 4, 3, 2, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 6, 2, 3, 2, 2, 6, 3, 5, 6, 1, 2, 3, 2, 2, 6, 4, 5, 6, 5, 5, 3, 2, 1, 1, 2, 1, 4, 5, 4, 4, 5, 4, 1, 1, 5, 0, 0, 0, 3, 1, 2, 0, 0, 0, 6, 5, 4, 0, 0, 0, 1, 2, 1, 0, 0, 0, 4, 4, 4, 0, 0, 0, 1, 1, 1, 0, 0, 0, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 5, 5, 4, 1, 2, 3, 2, 2, 6, 4, 5, 6, 2, 5, 3, 0, 0, 3, 1, 2, 0, 0, 0, 6, 1, 5, 0, 4, 5, 3, 1, 2, 3, 1, 2, 6, 1, 5, 6, 5, 2, 3, 1, 2, 6, 2, 5, 6, 1, 5, 3, 5, 5, 1, 1, 1, 3, 2, 2, 4, 4, 4, 6, 5, 5, 3, 2, 2, 3, 2, 2, 6, 2, 5, 6, 1, 1, 3, 2, 2, 4, 4, 4, 3, 2, 2, 1, 1, 1, 1, 2, 1, 4, 4, 4, 1, 2, 1, 1, 1, 1, 0, 0, 0, 4, 4, 4, 0, 0, 0, 1, 1, 1, 0, 0, 0, 4, 4, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 2, 2, 1, 1, 1, 3, 2, 2, 4, 1, 2, 6, 2, 5, 6, 4, 5, 3, 2, 2, 3, 0, 0, 6, 4, 5, 0, 0, 0, 3, 1, 2, 0, 4, 5, 6, 4, 5, 3, 1, 2, 3, 1, 2, 6, 5, 2, 6, 4, 5, 6, 2, 5, 3, 1, 2, 3, 5, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 5, 5, 6, 2, 5, 3, 2, 2, 3, 2, 2, 6, 5, 2, 6, 5, 5, 6, 3, 5, 3, 2, 2, 3, 2, 2, 4, 5, 4, 6, 2, 5, 1, 2, 1, 3, 2, 5, 4, 5, 4, 3, 5, 2, 1, 2, 1, 6, 5, 2, 4, 5, 4, 6, 2, 5, 1, 2, 1, 3, 2, 5, 4, 4, 4, 3, 2, 2, 1, 1, 1, 6, 2, 5, 6, 5, 5, 3, 6, 2, 3, 2, 2, 6]\n",
      "comparison with policy iteration: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_optimal,v_optimal = ValueIteration(0,S,A,0.9)\n",
    "print(\"optimal policy using value iteration is: \")\n",
    "print(pi_optimal)\n",
    "\n",
    "print(\"comparison with policy iteration: \")\n",
    "print(pi_optimal - pi_optimal)\n",
    "s_traj = trajectoryFun(0,pi_optimal,(1,4,6),S,A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 1:\n",
    "The initial policy will only converge with high probability when pe <= 0.1. For pe = 0.25, the initial policy will not converge because it will stay at the original position to avoid making mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 2: \n",
    "\n",
    "Assume the reward of +1 only applies when the robot is pointing down, e.g. h ∈ {5, 6, 7} in the goal square; the\n",
    "reward is 0 otherwise. Recompute trajectories and values given initial conditions from 3(c) with pe ∈ {0, 25%}. If pe = 0, then the trajecotry will not change since based on previous reward function, by using optimal policy, the robot will have a heading {7} at reward state, which is included in current reward heading h. If pe = 0.25, the robot will stay still because it doesn't want to risk taking actions as expected loss will be great.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardFun(s,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: This function takes a certain state in state map and output the reward at that state\n",
    "    \n",
    "    Input: s = current state, myStates = class of defined states\n",
    "    \n",
    "    Return: float(reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from states\n",
    "    S = myStates.stateMatrix\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    \n",
    "    \n",
    "    x_pos = s[0]\n",
    "    y_pos = s[1]\n",
    "    h = s[2]\n",
    "    \n",
    "    if x_pos < 0 or x_pos >= L or y_pos < 0 or y_pos >= W or h < 0 or h >= 12:\n",
    "        raise Exception('Invalid state definition: [x,y,h] should be within range')\n",
    "    \n",
    "    pos = [x_pos,y_pos]\n",
    "    \n",
    "    if x_pos == 0 or y_pos == 0 or x_pos == (L-1) or y_pos == (W-1):\n",
    "        r = -100\n",
    "    elif pos == [2,2] or pos == [2,3] or pos == [2,4] or pos == [4,2] or pos == [4,3] or pos == [4,4]:\n",
    "        r = -10\n",
    "        \n",
    "    # modification to original rewardFun\n",
    "    elif pos == [3,4] and h in [6,7,8]:\n",
    "        r = 1\n",
    "    else: r = 0\n",
    "     \n",
    "    # print(\"reward for state (%d,%d,%d) is %d\" %(s[0],s[1],s[2],r))\n",
    "    return r\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iteration done in 63.299370 seconds\n",
      "recomputed trajectory using new reward function: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pi_optimal,v_optimal = policyIteration(0,p_init,0.9,S,A)\n",
    "s_traj = trajectoryFun(0,pi_optimal,(1,4,6),S,A)\n",
    "print(\"recomputed trajectory using new reward function with pe = 0: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 3: Discussion\n",
    "\n",
    "Theoretically, both value iteration process and policy iteration process will converges the optimal policy and optimal value to the same quantities. If the number of states are not large, their computational time will not have big difference. \n",
    "\n",
    "\n",
    "If we change lane penalty from -10 to -1 in map, it might not be intuitive that the robot will go across a path with -1 reward (across lane) and then reach +1 reward in the following map. However, this is reasonable becuase a -1 lane penalty is not large enough to decrease the expectation value for robot taking action to cross lane and get reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardFun(s,myStates):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: This function takes a certain state in state map and output the reward at that state\n",
    "    \n",
    "    Input: s = current state, myStates = class of defined states\n",
    "    \n",
    "    Return: float(reward)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from states\n",
    "    S = myStates.stateMatrix\n",
    "    L = myStates.L\n",
    "    W = myStates.W\n",
    "    \n",
    "    \n",
    "    x_pos = s[0]\n",
    "    y_pos = s[1]\n",
    "    h = s[2]\n",
    "    \n",
    "    if x_pos < 0 or x_pos >= L or y_pos < 0 or y_pos >= W or h < 0 or h >= 12:\n",
    "        raise Exception('Invalid state definition: [x,y,h] should be within range')\n",
    "    \n",
    "    pos = [x_pos,y_pos]\n",
    "    \n",
    "    if x_pos == 0 or y_pos == 0 or x_pos == (L-1) or y_pos == (W-1):\n",
    "        r = -100\n",
    "        \n",
    "    # modified reward at lane to be -1\n",
    "    elif pos == [2,2] or pos == [2,3] or pos == [2,4] or pos == [4,2] or pos == [4,3] or pos == [4,4]:\n",
    "        r = -1\n",
    "    elif pos == [3,4]:\n",
    "        r = 1\n",
    "    else: r = 0\n",
    "     \n",
    "    # print(\"reward for state (%d,%d,%d) is %d\" %(s[0],s[1],s[2],r))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iterates for 1 times......\n",
      "policy iterates for 2 times......\n",
      "policy iterates for 3 times......\n",
      "policy iterates for 4 times......\n",
      "policy iterates for 5 times......\n",
      "policy iterates for 6 times......\n",
      "policy iterates for 7 times......\n",
      "policy iterates for 8 times......\n",
      "policy iterates for 9 times......\n",
      "policy iterates for 10 times......\n",
      "policy iteration done in 62.710442 seconds\n",
      "recomputed trajectory using new reward function with pe = 0: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_optimal,v_optimal = policyIteration(0,p_init,0.9,S,A)\n",
    "s_traj = trajectoryFun(0,pi_optimal,(1,4,6),S,A)\n",
    "print(\"recomputed trajectory using new reward function with pe = 0: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
